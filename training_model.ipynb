{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6edcd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca18ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac10101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07f1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13cfc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2f6f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_Net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0360fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c027887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('image_bin.npy', 'rb') as f:\n",
    "    img_data = np.load(f)\n",
    "with open('class_bin.npy', 'rb') as c:\n",
    "    class_data = np.load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440f52a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 240, 640)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d14a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ceeaccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afcfcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromImages(Dataset):\n",
    "    def __init__(self, img_data, cl_data):\n",
    "        self.image_arr = img_data\n",
    "        self.cl_arr = cl_data - 1\n",
    "        self.data_len = len(self.image_arr)\n",
    "    def __getitem__(self, index):\n",
    "        img = np.asarray(self.image_arr[index])\n",
    "        img = torch.as_tensor(img)/255\n",
    "        img = img.unsqueeze(0)\n",
    "        \n",
    "        img = func.resize(img, size=[256, 256])\n",
    "        \n",
    "        cl = np.asarray(self.cl_arr[index])\n",
    "        cl = torch.as_tensor(cl)\n",
    "        \n",
    "        return (img.float(), cl.type(torch.LongTensor))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be2816f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(img_data, class_data,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e2cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDatasetFromImages(x_train, y_train)\n",
    "test_data = CustomDatasetFromImages(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c5900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_data,batch_size=100,shuffle=True)\n",
    "test_data_loader = DataLoader(test_data,batch_size=100,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "258b8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "174a25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 7, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(7, 10, 3)\n",
    "        self.fc1 = nn.Linear(38440, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97866fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13a1593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 7, 252, 252]             182\n",
      "         MaxPool2d-2          [-1, 7, 126, 126]               0\n",
      "            Conv2d-3         [-1, 10, 124, 124]             640\n",
      "         MaxPool2d-4           [-1, 10, 62, 62]               0\n",
      "            Linear-5                  [-1, 120]       4,612,920\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 4,624,756\n",
      "Trainable params: 4,624,756\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 5.71\n",
      "Params size (MB): 17.64\n",
      "Estimated Total Size (MB): 23.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2988a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb2a2e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armik\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.306\n",
      "[1,   100] loss: 2.304\n",
      "[1,   150] loss: 2.304\n",
      "[2,    50] loss: 2.302\n",
      "[2,   100] loss: 2.299\n",
      "[2,   150] loss: 2.298\n",
      "[3,    50] loss: 2.292\n",
      "[3,   100] loss: 2.284\n",
      "[3,   150] loss: 2.271\n",
      "[4,    50] loss: 2.234\n",
      "[4,   100] loss: 2.137\n",
      "[4,   150] loss: 1.750\n",
      "[5,    50] loss: 0.935\n",
      "[5,   100] loss: 0.571\n",
      "[5,   150] loss: 0.338\n",
      "[6,    50] loss: 0.191\n",
      "[6,   100] loss: 0.119\n",
      "[6,   150] loss: 0.081\n",
      "[7,    50] loss: 0.064\n",
      "[7,   100] loss: 0.061\n",
      "[7,   150] loss: 0.042\n",
      "[8,    50] loss: 0.035\n",
      "[8,   100] loss: 0.025\n",
      "[8,   150] loss: 0.029\n",
      "[9,    50] loss: 0.018\n",
      "[9,   100] loss: 0.016\n",
      "[9,   150] loss: 0.014\n",
      "[10,    50] loss: 0.010\n",
      "[10,   100] loss: 0.008\n",
      "[10,   150] loss: 0.010\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eb0efac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.003. Test acc: 0.998\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
    "for j, data in enumerate(test_data_loader, 0):\n",
    "    test_img, test_labels = data\n",
    "    test_outputs = net(test_img)\n",
    "    test_preds = torch.max(test_outputs, dim=1)\n",
    "    # подсчет ошибки на тесте\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "        # подсчет метрики на тесте\n",
    "    test_running_total += len(test_labels)\n",
    "    pred_test_labels = test_preds.indices\n",
    "    test_running_right += (test_labels == pred_test_labels).sum()\n",
    "    preds.append(pred_test_labels)\n",
    " \n",
    "print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9662930",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'gesture_classification_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e843e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
